{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a687bb0",
   "metadata": {},
   "source": [
    "# OCI Data Science - Useful Tips\n",
    "<details>\n",
    "<summary><font size=\"2\">Check for Public Internet Access</font></summary>\n",
    "\n",
    "```python\n",
    "import requests\n",
    "response = requests.get(\"https://oracle.com\")\n",
    "assert response.status_code==200, \"Internet connection failed\"\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Helpful Documentation </font></summary>\n",
    "<ul><li><a href=\"https://docs.cloud.oracle.com/en-us/iaas/data-science/using/data-science.htm\">Data Science Service Documentation</a></li>\n",
    "<li><a href=\"https://docs.cloud.oracle.com/iaas/tools/ads-sdk/latest/index.html\">ADS documentation</a></li>\n",
    "</ul>\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Typical Cell Imports and Settings for ADS</font></summary>\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)\n",
    "\n",
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.automl.provider import OracleAutoMLProvider\n",
    "from ads.automl.driver import AutoML\n",
    "from ads.evaluations.evaluator import ADSEvaluator\n",
    "from ads.common.data import ADSData\n",
    "from ads.explanations.explainer import ADSExplainer\n",
    "from ads.explanations.mlx_global_explainer import MLXGlobalExplainer\n",
    "from ads.explanations.mlx_local_explainer import MLXLocalExplainer\n",
    "from ads.catalog.model import ModelCatalog\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "```\n",
    "</details>\n",
    "<details>\n",
    "<summary><font size=\"2\">Useful Environment Variables</font></summary>\n",
    "\n",
    "```python\n",
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3c719",
   "metadata": {},
   "source": [
    "### 1. Set up spark context and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c75424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3362c7",
   "metadata": {},
   "source": [
    "# VALIDATION_FILE_PATH: Validation data path that needs to be predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b534f53",
   "metadata": {},
   "source": [
    "### Running whole code from beginning to end may last 18 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dd3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data path that contains data to predict\n",
    "VALIDATION_DATA_PATH = \"TICKET_VALIDATION_DATA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af651662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final path that will contain predicted values.\n",
    "VALIDATION_DATA_OUTPUT_PATH = 'FRP_hamsi.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fcf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"TICKET.csv\"\n",
    "\n",
    "TRAINING_DATA_PATH = \"TICKET_TRAIN.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ea8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ec71fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/datascience/spark_conf_dir/oci-hdfs-full-3.3.0.6.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Ucuş tipi tahminleme multinomial logical regression\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#.config(\"spark.some.config.option\", \"some.value\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfce436",
   "metadata": {},
   "source": [
    "### 2. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6c5752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# datas to train the model\n",
    "df_train = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(TRAINING_DATA_PATH, header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "682ce083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# reading validation data\n",
    "df_validation = spark.read.format('com.databricks.spark.csv') \\\n",
    "            .options(header='true', inferschema='true') \\\n",
    "            .load(VALIDATION_DATA_PATH, header=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5acd874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_train.union(df_validation)\n",
    "\n",
    "#df.show(5)\n",
    "\n",
    "#producing the dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43128b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, array, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64337297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df.filter(col(\"FLIGHT_REASON\") != \"NONE\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\\'TICKET_TRAIN.csv\\')\\ndf.filter(col(\"FLIGHT_REASON\") == \"NONE\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\\'TICKET_VALIDATION.csv\\')'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df.filter(col(\"FLIGHT_REASON\") != \"NONE\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save('TICKET_TRAIN.csv')\n",
    "df.filter(col(\"FLIGHT_REASON\") == \"NONE\").repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save('TICKET_VALIDATION.csv')\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af321850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting datas on OND column\n",
    "def ond_to_dep(x):\n",
    "    return str(str(x).split(\"-\")[0])\n",
    "\n",
    "def ond_to_arr(x):\n",
    "    return str(str(x).split(\"-\")[1])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "\n",
    "ond_to_dep_udf = udf(lambda x: ond_to_dep(x), StringType())\n",
    "df = df.withColumn(\"DEP_CITY\", ond_to_dep_udf(\"OND\"))\n",
    "\n",
    "ond_to_arr_udf = udf(lambda x: ond_to_arr(x), StringType())\n",
    "df = df.withColumn(\"ARR_CITY\", ond_to_arr_udf(\"OND\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd44e237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "\n",
    "df_stats = df.select(\n",
    "    _mean(col('CUST_AGE')).alias('mean')\n",
    ").collect()\n",
    "\n",
    "year_mean = df_stats[0]['mean']\n",
    "#year_std = df_stats[0]['std']\n",
    "\n",
    "df = df.fillna(year_mean, subset=[\"CUST_AGE\"])\n",
    "\n",
    "# empty cells on CUST_AGE are fillling with average value of CUST_AGE column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f070b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"NTNLT1\") # Removing NTNLT1 column since most of it's cells are N/A\n",
    "\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e516387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLIGHT_REASON</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEISURE</td>\n",
       "      <td>576739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECOND HOME</td>\n",
       "      <td>259011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDENT</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>458593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NONE</td>\n",
       "      <td>8911797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FLIGHT_REASON    count\n",
       "0       LEISURE   576739\n",
       "1   SECOND HOME   259011\n",
       "2       STUDENT     1228\n",
       "3      BUSINESS   458593\n",
       "4          NONE  8911797"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"FLIGHT_REASON\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abacdc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:==========================================>             (12 + 4) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 2770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# copying STUDENT flight type rows on FLIGHT_REASON column because there's not many STUDENT flight type when compared with other ones\n",
    "\n",
    "from pyspark.sql.functions import col, explode, array, lit\n",
    "\n",
    "major_df = df.filter(col(\"FLIGHT_REASON\") != \"STUDENT\")\n",
    "minor_df = df.filter(col(\"FLIGHT_REASON\") == \"STUDENT\")\n",
    "ratio = int(major_df.count()/minor_df.count()/3)\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bac7c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.039738893508911\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "a = range(ratio)\n",
    "# duplicate the minority rows\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "\n",
    "# combine both oversampled minority rows and previous majority rows \n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "df = combined_df\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43cd94c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FLIGHT_REASON</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEISURE</td>\n",
       "      <td>576739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECOND HOME</td>\n",
       "      <td>259011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>STUDENT</td>\n",
       "      <td>3401560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>458593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NONE</td>\n",
       "      <td>8911797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FLIGHT_REASON    count\n",
       "0       LEISURE   576739\n",
       "1   SECOND HOME   259011\n",
       "2       STUDENT  3401560\n",
       "3      BUSINESS   458593\n",
       "4          NONE  8911797"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"FLIGHT_REASON\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df442570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to float format\n",
    "def string_to_float(x):\n",
    "    return float(x)\n",
    "\n",
    "def date_to_year(x):\n",
    "    return int(str(x)[0:4])\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "\n",
    "\n",
    "date_to_year_udf = udf(lambda x: date_to_year(x), IntegerType())\n",
    "df = df.withColumn(\"ID_PNR_CREATION_YEAR\", date_to_year_udf(\"ID_PNR_CREATION_YMD\"))\n",
    "\n",
    "# producing ID_PNR_CREATION_YEAR column with using ID_PNR_CREATION_YMD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e2d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function which converts the columns hasn't digital values to a shape that StringIndexer and VectorIndexer can understand\n",
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols ]\n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(),outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=\"features\")\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "    return data.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b7c8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "categoricalCols = [\"DEP_CITY\",\"ARR_CITY\"]\n",
    "continuousCols = ['PNR_PSSG_COUNT','CHILD_FLG','INFANT_FLG','POS_POC_SAME_FLG','SAME_SRNAME_FLG','FAMILY_FLG','SEAT_SELECT_FLG','XBAG_FIRST_FLT_FLG','XBAG_LAST_FLT_FLG','XBAG_TWO_WAY_FLT_FLG','SPORT_FLG','XBAG_FLG','PET_FLG','CUST_AGE','ID_PNR_CREATION_YEAR']\n",
    "labelCol = \"FLIGHT_REASON\"\n",
    "\n",
    "# columns that are useful for machine learning\n",
    "\n",
    "df_features =  get_dummy(df,categoricalCols,continuousCols,labelCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18168562",
   "metadata": {},
   "source": [
    "### 4. Transform the dataset to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f437778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!!caution: NOT from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6416b3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------------+\n",
      "|            features|   label|indexedLabel|\n",
      "+--------------------+--------+------------+\n",
      "|(151,[16,68,136,1...|BUSINESS|         3.0|\n",
      "|(151,[0,73,136,13...|BUSINESS|         3.0|\n",
      "|(151,[0,82,136,14...|BUSINESS|         3.0|\n",
      "|(151,[1,68,136,14...|BUSINESS|         3.0|\n",
      "|(151,[0,112,136,1...|BUSINESS|         3.0|\n",
      "+--------------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index labels, adding metadata to the label column\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol='label', outputCol='indexedLabel').fit(df_features)\n",
    "\n",
    "labelIndexer.transform(df_features).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "660319f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+\n",
      "|            features|   label|     indexedFeatures|\n",
      "+--------------------+--------+--------------------+\n",
      "|(151,[16,68,136,1...|BUSINESS|(151,[16,68,136,1...|\n",
      "|(151,[0,73,136,13...|BUSINESS|(151,[0,73,136,13...|\n",
      "|(151,[0,82,136,14...|BUSINESS|(151,[0,82,136,14...|\n",
      "|(151,[1,68,136,14...|BUSINESS|(151,[1,68,136,14...|\n",
      "|(151,[0,112,136,1...|BUSINESS|(151,[0,112,136,1...|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 642, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(df_features)\n",
    "featureIndexer.transform(df_features).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64b12534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "154bde65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 642, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/datascience/conda/pyspark30_p37_cpu_v2/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "df_features = df_features.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "df = df.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "df_features_cols = df_features.columns\n",
    "df_features_cols.remove('id')\n",
    "\n",
    "df_cols = df.columns\n",
    "df_cols.remove('id')\n",
    "\n",
    "df_merged = df.join(df_features, df[\"id\"] == df_features[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d740ce2",
   "metadata": {},
   "source": [
    "### 5. Split the data to training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70a2ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is splitting to train and validation\n",
    "\n",
    "(df_merged_train, df_merged_test) = df_merged.filter(df_merged[\"FLIGHT_REASON\"] != \"NONE\"), df_merged.filter(df_merged[\"FLIGHT_REASON\"] == \"NONE\")\n",
    "\n",
    "\n",
    " \n",
    "(trainingData, testData) = df_merged.filter(df_merged[\"FLIGHT_REASON\"] != \"NONE\").select(df_features_cols), df_merged.filter(df_merged[\"FLIGHT_REASON\"] == \"NONE\").select(df_features_cols)\n",
    "(trainingDF, testDF) = df_merged.filter(df_merged[\"FLIGHT_REASON\"] != \"NONE\").select(df_cols), df_merged.filter(df_merged[\"FLIGHT_REASON\"] == \"NONE\").select(df_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea33534",
   "metadata": {},
   "source": [
    "### 6. Fit Multinomial logisticRegression Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1de81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying logistic regression since the value we need to predict has only four possible result.\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol = 'indexedLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604d956",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd0e0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol = \"predictedLabel\",labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bcbb46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "843ef9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Training the model with train data.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ec77d",
   "metadata": {},
   "source": [
    "### 8. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31a3e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc4ac1",
   "metadata": {},
   "source": [
    "### 9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b03088",
   "metadata": {},
   "source": [
    "#### Şimdi elimizde validasyon verisinin her bir satırı için bir tahmin var. Ancak bu veri \n",
    "#### ile validasyon verisi ayrıldığı için csv dosyasına yazdırmak mümkün olmadığından dolayı \n",
    "#### iki DataFrame'i birleştirmemiz gerek. Böylece csv dosyasına yazdırma işlemi tamamlanabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2a08ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = testDF.withColumn(\"id2\", monotonically_increasing_id())\n",
    "\n",
    "predictions = predictions.withColumn(\"id2\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "257a9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalTest = testDF.select(['PNR_NO', 'ID_TKT_NO','ID_PNR_CREATION_YMD','id2']).join(predictions.select('predictedLabel','id2'), testDF[\"id2\"] == predictions[\"id2\"])\n",
    "\n",
    "finalTest = finalTest.withColumnRenamed('predictedLabel','FLIGHT_REASON')\n",
    "\n",
    "finalTest = finalTest.drop(\"id2\")\n",
    "finalTest = finalTest.sort(finalTest.ID_TKT_NO.asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04e74f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "finalTest.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(VALIDATION_DATA_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba928b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
